{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: My name is Clara and I am\n",
      "Answer: Let me provide a detailed response: Clara is a girl's name. It is of Latin origin and means \"clear\". Clara is derived from the Latin word clara which means \"clear\". Clara is also a diminutive of the Latin word clarus meaning \"clear\". Clara is a short form of the names Clarissa, Clarice, and Claribel. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both girls and boys. Clara is also a very popular name in other countries such as Canada and Spain. Clara is a very popular name in the United States and is used for both\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set HuggingFace token\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Set device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# Directory where the model and tokenizer are saved\n",
    "saved_model_directory = \"F:\\LLM\\model\"  # Replace this with your model's saved path\n",
    "\n",
    "# Load the tokenizer from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(saved_model_directory)\n",
    "\n",
    "# Set up padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Load the model from the saved directory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    saved_model_directory,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=500,\n",
    "    min_new_tokens=30,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# Initialize the LangChain pipeline\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = PromptTemplate.from_template(\"\"\"Question: {question}\n",
    "Answer: Let me provide a detailed response:\"\"\")\n",
    "\n",
    "# Create output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# Run the chain with error handling\n",
    "try:\n",
    "    question = \"My name is Clara and I am\"\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Error during inference: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
